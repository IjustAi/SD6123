# -*- coding: utf-8 -*-
"""Copy of Comparison_CNN_vs_TF_v1_x_Example_for_CIFAR_10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1USk9SFNMiXSn2zZxAMlOlBST5xl238yU
"""

!pip install datasets

import numpy as np
from keras.optimizers import SGD
from keras.regularizers import l2
from tensorflow import keras
from tensorflow.nn import local_response_normalization
from keras.utils import to_categorical
import matplotlib.pyplot as plt

from datasets import load_dataset

dataset = load_dataset("flwrlabs/fed-isic2019")
print(dataset)

from tensorflow import keras
from keras.optimizers import SGD
from keras.regularizers import l2
from tensorflow.nn import local_response_normalization

def tf_example(input_shape, num_classes):
    """CNN Model (based on original CIFAR10 CNN from FedAvg paper)."""
    input_shape = tuple(input_shape)
    weight_decay = 0.004

    model = keras.Sequential([
        keras.layers.Conv2D(
            64, (5, 5), padding="same", activation="relu",
            input_shape=input_shape
        ),
        keras.layers.MaxPooling2D(pool_size=(3, 3), strides=2, padding="same"),
        keras.layers.Lambda(
            local_response_normalization,
            arguments={
                "depth_radius": 4,
                "bias": 1.0,
                "alpha": 0.001 / 9.0,
                "beta": 0.75,
            },
        ),
        keras.layers.Conv2D(
            64, (5, 5), padding="same", activation="relu"
        ),
        keras.layers.Lambda(
            local_response_normalization,
            arguments={
                "depth_radius": 4,
                "bias": 1.0,
                "alpha": 0.001 / 9.0,
                "beta": 0.75,
            },
        ),
        keras.layers.MaxPooling2D(pool_size=(3, 3), strides=2, padding="same"),
        keras.layers.Flatten(),
        keras.layers.Dense(
            384, activation="relu", kernel_regularizer=l2(weight_decay)
        ),
        keras.layers.Dense(
            192, activation="relu", kernel_regularizer=l2(weight_decay)
        ),
        keras.layers.Dense(num_classes, activation="softmax")
    ])

    optimizer = SGD(learning_rate=0.01, momentum=0.9)
    model.compile(
        loss="categorical_crossentropy",
        optimizer=optimizer,
        metrics=["accuracy"]
    )

    return model

def load_fed_isic2019_all_clients(num_classes=None, input_shape=None, img_size=(224, 224)):
    print(f">>> [Dataset] Loading FED-ISIC2019 (merged clients). Resize to {img_size}")

    dataset = load_dataset("flwrlabs/fed-isic2019")
    train_data = dataset["train"]
    test_data = dataset["test"]

    def preprocess(example):
        image = example["image"].convert("RGB")
        image = image.resize(img_size)
        image = np.asarray(image).astype("float32") / 255.0

        label = example["label"]
        if label is None or label == -1:
            return None
        return image, int(label)

    train = [preprocess(ex) for ex in train_data if preprocess(ex) is not None]
    x_train, y_train = zip(*train)

    test = [preprocess(ex) for ex in test_data if preprocess(ex) is not None]
    x_test, y_test = zip(*test)

    x_train = np.stack(x_train)
    y_train = np.array(y_train, dtype=np.int32)
    x_test = np.stack(x_test)
    y_test = np.array(y_test, dtype=np.int32)

    input_shape = x_train.shape[1:]
    num_classes = len(np.unique(y_train))

    print(f">>> Done. Input shape: {input_shape} | Num classes: {num_classes}")
    print(f">>> Train label distribution: {np.unique(y_train, return_counts=True)}")
    print(f">>> Test label distribution: {np.unique(y_test, return_counts=True)}")

    return x_train, y_train, x_test, y_test, input_shape, num_classes

x_train, y_train, x_test, y_test, input_shape,num_classes = load_fed_isic2019_all_clients(img_size=(32,32))



EPOCHS=350
BATCH_SIZE=128

"""---"""

model = tf_example(input_shape, num_classes)

history = model.fit(x_train, to_categorical(y_train, num_classes), epochs=EPOCHS, batch_size=BATCH_SIZE)

loss = history.history['loss']
epochs = range(1, len(loss) + 1)

plt.plot(epochs, loss, 'b', label='Training Loss')
plt.title('Training Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

ac = history.history['accuracy']
epochs = range(1, len(ac) + 1)

plt.plot(epochs, ac, 'b', label='Training AC')
plt.title('Training AC')
plt.xlabel('Epochs')
plt.ylabel('AC')
plt.legend()
plt.show()

from keras.utils import to_categorical
y_test_cat = to_categorical(y_test, num_classes)
model.evaluate(x_test, y_test_cat)

"""---"""

from tensorflow import keras
from keras.optimizers import SGD
from keras.regularizers import l2

def cnn(input_shape, num_classes):
    """Improved CNN Model based on FedAvg (McMahan et al. 2017)"""
    input_shape = tuple(input_shape)
    weight_decay = 0.004

    model = keras.Sequential([
        keras.layers.Conv2D(
            64, (5, 5), padding="same", activation="relu", input_shape=input_shape
        ),
        keras.layers.BatchNormalization(),
        keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),

        keras.layers.Conv2D(
            64, (5, 5), padding="same", activation="relu"
        ),
        keras.layers.BatchNormalization(),
        keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),

        keras.layers.Flatten(),
        keras.layers.Dense(384, activation="relu", kernel_regularizer=l2(weight_decay)),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(192, activation="relu", kernel_regularizer=l2(weight_decay)),
        keras.layers.Dropout(0.3),
        keras.layers.Dense(num_classes, activation="softmax"),
    ])

    optimizer = SGD(learning_rate=0.01, momentum=0.9)
    model.compile(
        loss="categorical_crossentropy",
        optimizer=optimizer,
        metrics=["accuracy"]
    )

    return model

model_cnn = cnn(input_shape, num_classes)

history_cnn = model_cnn.fit(x_train, to_categorical(y_train, num_classes), epochs=350, batch_size=100)

model_cnn.evaluate(x_test, to_categorical(y_test, num_classes))

loss = history_cnn.history['loss']
epochs = range(1, len(loss) + 1)

plt.plot(epochs, loss, 'b', label='Training Loss')
plt.title('Training Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

ac = history_cnn.history['accuracy']
epochs = range(1, len(ac) + 1)

plt.plot(epochs, ac, 'b', label='Training AC')
plt.title('Training AC')
plt.xlabel('Epochs')
plt.ylabel('AC')
plt.legend()
plt.show()